\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{xcolor}
\pgfplotsset{compat=1.18}

\title{Bayesian Projection of Developer Productivity Gains\\from OSS Vulnerability Scanning and Veracode Pipeline Scans}
\author{dbSAIcle Research}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
This paper presents a Bayesian projection of developer productivity gains from two automated security tools embedded in dbSAIcle: an OSS vulnerability scan and a Veracode pipeline scan. We outline an evidence plan (scan logs, findings JSON, fix commits, CI logs) and use a hierarchical Bayesian model to estimate expected reductions in time-to-remediate (TTR), vulnerability backlog, release gate failures, and context switching. A one-month scenario (two sprints) illustrates projected gains and yields posterior probabilities of improvement and economic ROI under stated assumptions. The result is a decision-ready forecast for shift-left remediation while dbSAIcle remains in stealth mode.
\end{abstract}

\section{Introduction}
Modern software development teams face mounting security obligations while maintaining delivery velocity. In many organizations, OSS and Veracode scans are performed at release time on production-bound artifacts. When high or critical findings appear, SDLC gates fail and releases are delayed. This can trigger missed business commitments, emergency change approvals, rollback risk, and reputational damage. Additional risk includes compliance breaches, SLA violations, and rushed hotfixes under time pressure.

Tool fragmentation adds friction. Developers must jump between build systems, security portals, and ticketing tools, which increases context switching and slows remediation. By embedding scans and remediation workflows inside dbSAIcle, the security loop is intended to move into the development phase and become repeatable.

This paper projects the impact of two dbSAIcle tools with a Bayesian framework:
\begin{itemize}
  \item \texttt{oss\_vulnerability\_scan}: detects dependency vulnerabilities and produces actionable remediation guidance.
  \item \texttt{veracode\_pipeline\_scan}: performs static analysis on built artifacts with rapid turnaround in CI.
\end{itemize}

We measure productivity gains using a planned proof package that will link each finding to scan IDs, artifact hashes, fix commits, and passing builds. The Bayesian model quantifies improvement probabilities and effect sizes, while a one-month scenario (two sprints) illustrates the magnitude of expected gains.

\section{Tools Under Study}
\subsection{OSS Vulnerability Scan}
The OSS scan tool ingests a project directory and performs dependency analysis for Maven, Gradle, or npm projects. It produces a Markdown report listing vulnerabilities by severity, affected versions, and recommended fixes.

\subsection{Veracode Pipeline Scan}
The Veracode tool uploads an artifact (JAR, WAR, EAR, ZIP, APK) to the pipeline scan API, polls scan status, and retrieves findings. It returns a Markdown report with findings by severity and location.

Both tools are designed to be available inside the dbSAIcle plugin and can be invoked with natural-language workflows, reducing context switching and enabling remediation during development.

\section{Research Questions}
\begin{enumerate}
  \item How much do the tools reduce TTR and backlog compared to release-time scanning baselines?
  \item How much do the tools reduce release gate failures and release delays per sprint?
  \item How much context switching time is avoided by running scans inside dbSAIcle?
  \item Is there a synergistic effect when both tools are used in the same sprint?
  \item What is the expected one-month ROI (two sprints) from measured hours saved?
\end{enumerate}

\section{Data and Measurements}
\subsection{Outcome Variables}
\begin{itemize}
  \item \textbf{TTR (Time-to-Remediate)}: time from detection to verified fix (hours or days).
  \item \textbf{Backlog Count}: unresolved vulnerabilities per sprint.
  \item \textbf{Rework Rate}: reopened fixes or follow-up PRs per vulnerability.
  \item \textbf{Release Gate Failures}: count of SDLC gate failures or release delays per sprint.
  \item \textbf{Context Switching Time}: minutes spent moving between tools per finding.
  \item \textbf{Cycle Time} (optional): time from PR open to merge.
\end{itemize}

\subsection{Treatment Variables}
\begin{itemize}
  \item \texttt{oss\_usage\_it}: count of OSS scans per project per sprint.
  \item \texttt{vera\_usage\_it}: count of Veracode pipeline scans per project per sprint.
  \item \texttt{oss\_adopt\_it}, \texttt{vera\_adopt\_it}: binary indicators for tool usage.
  \item Interaction: \texttt{oss\_usage\_it * vera\_usage\_it}.
\end{itemize}

\subsection{Control Variables}
\begin{itemize}
  \item Project size (LOC, module count, dependency count).
  \item Team size, sprint length, release cadence.
  \item Language and build system.
  \item Time effects (calendar week or sprint).
\end{itemize}

\subsection{Data Sources}
\begin{itemize}
  \item Tool telemetry logs (scan timestamps, scan IDs, findings count).
  \item Issue tracker data (vulnerability tickets and resolution timestamps).
  \item CI/CD logs (build time and artifact metadata).
  \item Repository analytics (PR cycle time, commit history).
  \item Release gate logs and change approval records.
\end{itemize}

\subsection{Evidence Artifacts (Proof Package)}
Each finding will be linked to concrete artifacts that provide auditability:
\begin{itemize}
  \item Scan request and response logs with scan IDs.
  \item Findings JSON stored with checksum and timestamp.
  \item Artifact metadata (name, size, hash) used in the scan.
  \item Fix commit IDs and PR links referencing the finding.
  \item CI build logs showing remediation validation.
  \item Release gate outcomes for the affected sprint.
\end{itemize}

\section{Methodology}
\subsection{Hierarchical Bayesian Model}
Let $TTR_{it}$ be time-to-remediate for project $i$ at time $t$. A log-normal model:
\[
\log(TTR_{it}) \sim \mathcal{N}(\mu_{it}, \sigma)
\]
\[
\mu_{it} = \alpha_{team[i]} + \alpha_{proj[i]} + \delta_t
          + \beta_{oss} \cdot oss\_usage_{it}
          + \beta_{vera} \cdot vera\_usage_{it}
          + \beta_{int} \cdot (oss\_usage_{it} \cdot vera\_usage_{it})
          + \gamma \cdot X_{it}
\]

Backlog count can be modeled with a Negative Binomial:
\[
backlog_{it} \sim \text{NegBin}(\lambda_{it}, \phi)
\]
\[
\log(\lambda_{it}) = \alpha_{team} + \alpha_{proj} + \delta_t +
\beta_{oss} \cdot oss\_usage_{it} + \beta_{vera} \cdot vera\_usage_{it} + \gamma \cdot X_{it}
\]

\subsection{Priors}
After standardizing predictors (mean 0, std 1):
\[
\beta_* \sim \mathcal{N}(0, 0.5)
\]
\[
\alpha_{team} \sim \mathcal{N}(0, \sigma_{team}), \quad \sigma_{team} \sim \text{HalfNormal}(0.5)
\]
\[
\alpha_{proj} \sim \mathcal{N}(0, \sigma_{proj}), \quad \sigma_{proj} \sim \text{HalfNormal}(0.5)
\]
\[
\sigma \sim \text{HalfNormal}(0.5), \quad \phi \sim \text{HalfNormal}(1.0)
\]

\subsection{Evidence Linkage Plan}
Each observation in the model is designed to trace an auditable chain: scan ID to findings JSON, findings to fix commit, and fix commit to passing CI build. This linkage is intended to provide proof that measured improvements are tied to verifiable artifacts and release gate outcomes once full data collection is complete.

\subsection{Causal Considerations}
To reduce selection bias:
\begin{itemize}
  \item Include time fixed effects ($\delta_t$) for calendar shocks.
  \item Use difference-in-differences when adoption is staged.
  \item Control for project and team random effects.
\end{itemize}

\section{Inference and Computation}
Inference can be performed using Stan or PyMC with 4 chains and 2000-4000 iterations. Convergence is validated using $\hat{R} < 1.01$ and sufficient effective sample sizes.

Posterior summaries to report:
\begin{itemize}
  \item $P(\beta_{oss} < 0)$ and $P(\beta_{vera} < 0)$ (improvement probability for TTR).
  \item Median and 95\% credible intervals for effect sizes.
  \item Expected hours saved per sprint.
\end{itemize}

\section{Projected Results: One-Month Scenario (Two Sprints)}
We modeled a one-month window (two sprints) across eight active repositories. The values below illustrate a plausible shift-left outcome under conservative assumptions and are presented as scenario projections while dbSAIcle is in stealth mode.

\begin{table}[h]
  \centering
  \begin{tabular}{lcc}
    \toprule
    Metric & Baseline (release-time scans) & With dbSAIcle shift-left \\
    \midrule
    High/critical findings reaching release gate & 12 & 3 \\
    Median TTR (days) & 6.0 & 2.4 \\
    Release gate failures (count) & 4 & 1 \\
    Backlog at sprint end & 18 & 7 \\
    Context switching per finding (min) & 40 & 15 \\
    \bottomrule
  \end{tabular}
  \caption{Illustrative one-month scenario (two sprints) for projected gains.}
\end{table}

\subsection{Bayesian Effect Estimates (Projected)}
\begin{itemize}
  \item $P(\beta_{oss} < 0) = 0.97$ (scenario projection)
  \item Median TTR reduction (OSS) = 58\%, 95\% CI [34\%, 71\%] (scenario)
  \item $P(\beta_{vera} < 0) = 0.95$ (scenario projection)
  \item Median TTR reduction (Veracode) = 41\%, 95\% CI [18\%, 58\%] (scenario)
  \item Interaction effect: $\beta_{int} = -0.11$, 95\% CI [$-0.19, -0.03$] (scenario)
\end{itemize}

\subsection{Productivity and ROI (Projected)}
Estimated developer hours saved per month: 120, 95\% CI [90, 165] (scenario). Using a blended cost rate from finance, the ROI distribution yields a median 2.6x return with a 95\% credible interval of [1.6x, 4.1x] under the same assumptions.

\section{Sample Figures}
\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}
\pgfmathdeclarefunction{lognormal}{2}{%
  \pgfmathparse{1/(x*#2*sqrt(2*pi))*exp(-((ln(x)-#1)^2)/(2*#2^2))}%
}

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.9\textwidth,
      height=6cm,
      xlabel={Standardized Effect Size},
      ylabel={Density},
      legend style={at={(0.98,0.98)},anchor=north east},
      grid=both
    ]
      \addplot [blue, thick, domain=-1.5:1.5, samples=200] {gauss(-0.20,0.20)};
      \addlegendentry{$\beta_{oss}$}
      \addplot [red, thick, domain=-1.5:1.5, samples=200] {gauss(-0.12,0.22)};
      \addlegendentry{$\beta_{vera}$}
    \end{axis}
  \end{tikzpicture}
\caption{Posterior densities for OSS and Veracode effects (illustrative scenario).}
\label{fig:posterior}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.9\textwidth,
      height=6cm,
      xlabel={ROI (normalized)},
      ylabel={Density},
      grid=both
    ]
      \addplot [green!60!black, thick, domain=0.05:6, samples=200] {lognormal(0.6,0.5)};
    \end{axis}
  \end{tikzpicture}
\caption{ROI distribution from the one-month scenario.}
\label{fig:roi}
\end{figure}

\section{Discussion}
The scenario projections and Bayesian estimates indicate potential improvements in remediation speed, backlog reduction, and release gate stability. The tools are designed to convert late-stage security risk into early, actionable work, while reducing context switching across multiple scan systems. The probabilistic results provide decision-grade guidance on expected impact while empirical evidence is being collected.

\section{Threats to Validity}
\begin{itemize}
  \item Selection bias: early adopters may already be more efficient.
  \item Measurement error: missing or inconsistent timestamps, mitigated by artifact linkage.
  \item External shocks: release pressure or policy changes.
  \item Model risk: projections depend on assumptions and priors.
\end{itemize}
Mitigations include fixed effects, hierarchical modeling, and sensitivity checks.

\section{Conclusion}
This paper provides a Bayesian projection of productivity benefits from OSS and Veracode tooling while dbSAIcle is in stealth mode. The framework delivers decision-grade estimates of expected improvement and ROI, and the planned proof package will connect findings to fixes and passing builds as evidence accumulates. The model is reusable for other developer tooling initiatives.

\appendix
\section{Example Data Schema}
\begin{itemize}
  \item \texttt{project\_id}, \texttt{team\_id}, \texttt{sprint\_id}
  \item \texttt{oss\_scans\_count}, \texttt{veracode\_scans\_count}
  \item \texttt{ttr\_hours}, \texttt{vuln\_backlog\_count}, \texttt{rework\_count}
  \item \texttt{loc}, \texttt{dependency\_count}, \texttt{team\_size}
\end{itemize}

\section{Analysis Pipeline (Sketch)}
\begin{enumerate}
  \item Extract telemetry and ticket data.
  \item Normalize fields and standardize predictors.
  \item Fit hierarchical model and validate convergence.
  \item Report posterior summaries and ROI distribution.
\end{enumerate}

\end{document}
